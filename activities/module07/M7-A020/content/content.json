{"taskDescription":"<p>Chapter 7 has shown you the two main neural architectures used for neural machine translation. Think of their similarities and differences before answering. Drag the words into the correct boxes.<\/p>\n","overallFeedback":[{"from":0,"to":100}],"checkAnswer":"Check","tryAgain":"Retry","showSolution":"Show solution","dropZoneIndex":"Drop Zone @index.","empty":"Drop Zone @index is empty.","contains":"Drop Zone @index contains draggable @draggable.","ariaDraggableIndex":"@index of @count draggables.","tipLabel":"Show tip","correctText":"Correct!","incorrectText":"Incorrect!","resetDropTitle":"Reset drop","resetDropDescription":"Are you sure you want to reset this drop zone?","grabbed":"Draggable is grabbed.","cancelledDragging":"Cancelled dragging.","correctAnswer":"Correct answer:","feedbackHeader":"Feedback","behaviour":{"enableRetry":true,"enableSolutionsButton":true,"enableCheckButton":true,"instantFeedback":false},"scoreBarLabel":"You got :num out of :total points","a11yCheck":"Check the answers. The responses will be marked as correct, incorrect, or unanswered.","a11yShowSolution":"Show the solution. The task will be marked with its correct solution.","a11yRetry":"Retry the task. Reset all responses and start the task over again.","textField":"Consider the encoder of a neural machine translation system. In neural networks known as *recurrent*, the attention used to compute the embeddings for one word is mostly put on the embeddings of the closer words in the source sentence. However, in the neural networks known as *transformers*, attention can be virtually put on the embeddings of every word in the source sentence."}